{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Załadowanie bibliotek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "import category_encoders as ce\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Zbiór danych \"apartments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(\"apartments.csv\")\n",
    "dtest = pd.read_csv(\"apartments_test.csv\")\n",
    "\n",
    "target = \"m2.price\"\n",
    "predictors = [i for i in dtrain.columns if i not in [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Sprawdzenie konieczności imputacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Brak braków danych mozemy przejść do encodingu zmiennych kategorycznych\n",
    "dtrain.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Podział danych na zbiór testowy oraz treningowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzmy jak duzo jest uniklanych wartości w kolumnei district\n",
    "dtrain[\"district\"].unique()\n",
    "# Jest ich na tyle nieduzo, ze mozęmy skorzystać z OneHotEncodingu\n",
    "\n",
    "X_train = dtrain[predictors]\n",
    "Y_train = dtrain[target]\n",
    "X_test = dtest[predictors]\n",
    "Y_test = dtest[target]\n",
    "\n",
    "# Kolumny z danymi numerycznymi\n",
    "num = X_train.select_dtypes(include=['float64', 'int']).columns\n",
    "\n",
    "# Kolumny z danymi kategorycznymi\n",
    "cat = X_train.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. SVR ze StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funkcja służąca do zbudowania pipeline'a, który w kolejnych krokach będzie miał encoding, scalling oraz \n",
    "# klasyfikator - dzięki temu pozbywamy sie dataleak'u przy kroswalidacji, ponieważ dla każdego podziału \n",
    "# kroswalidacji będzie wykonywany scalling tylko na zbiorze treningowym, nie biorąc pod uwagę testowego\n",
    "def makePipeline(cat_indices, num_indices, classificator, encoder, scaled = False):\n",
    "    \n",
    "    if scaled == True:\n",
    "        pipeline = Pipeline(steps = [\n",
    "        ('feature_processing', FeatureUnion(transformer_list = [\n",
    "                ('categorical', FunctionTransformer(lambda data: data[cat_indices])),\n",
    "\n",
    "                #numeric\n",
    "                ('numeric', Pipeline(steps = [\n",
    "                    ('select', FunctionTransformer(lambda data: data[num_indices])),\n",
    "                    ('scale', StandardScaler())\n",
    "                            ]))\n",
    "            ])),\n",
    "        ('encoder', encoder),\n",
    "        ('classifier', classificator)\n",
    "        ]\n",
    "    )\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "        ('encoder', encoder),\n",
    "        ('classifier', classificator)\n",
    "        ])\n",
    "    return pipeline\n",
    "\n",
    "# Funkcja, która zmierzy nam ilość czasu potrzebną na wykonanie danej funkcji\n",
    "\n",
    "def timer(start_time = None):\n",
    "    if not start_time:\n",
    "        start_time = datetime.now()\n",
    "        return start_time\n",
    "    elif start_time:\n",
    "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
    "        tmin, tsec = divmod(temp_sec, 60)\n",
    "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 6 minutes and 17.46 seconds.\n",
      "-45.98750420537989\n",
      "{'classifier__gamma': 'scale', 'classifier__degree': 9, 'classifier__C': 21.01}\n"
     ]
    }
   ],
   "source": [
    "# Stworzenei Pipeline'a z domyślnym klasyfiaktorem\n",
    "svr_classifier = svm.SVR()\n",
    "svr_scaled_pipeline = makePipeline(cat, num, svr_classifier, ce.OneHotEncoder(), True)\n",
    "\n",
    "params = {\n",
    "    \"classifier__C\": [i/100 for i in range(1,100000,100)],\n",
    "    \"classifier__gamma\": [\"scale\",\"auto\"],\n",
    "    \"classifier__degree\": [i for i in range(1,10,1)]\n",
    "}\n",
    "\n",
    "# Jako metryke wzgledem ktorej bedziemy powownywac przyjmiemy RMSE\n",
    "svr_scaled_random_search = RandomizedSearchCV(svr_scaled_pipeline, param_distributions = params, n_iter = 1000,\n",
    "                                      scoring = 'neg_root_mean_squared_error', n_jobs = -1, cv = 10,\n",
    "                                      random_state = 1001)\n",
    "\n",
    "start_time = timer(None) # aktualna godzina\n",
    "svr_scaled_random_search.fit(X_train, Y_train)\n",
    "timer(start_time) # ile czasu minęło od start_time\n",
    "print(svr_scaled_random_search.best_score_)\n",
    "print(svr_scaled_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. SVR bez StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 5 minutes and 7.42 seconds.\n",
      "-772.0637357973118\n",
      "{'classifier__gamma': 'auto', 'classifier__degree': 7, 'classifier__C': 999.01}\n"
     ]
    }
   ],
   "source": [
    "# Stworzenei Pipeline'a z domyślnym klasyfiaktorem\n",
    "svr_classifier = svm.SVR()\n",
    "svr_pipeline = makePipeline(cat, num, svr_classifier, ce.OneHotEncoder(), False)\n",
    "\n",
    "# Jako metryke wzgledem ktorej bedziemy powownywac przyjmiemy RMSE\n",
    "svr_random_search = RandomizedSearchCV(svr_pipeline, param_distributions = params, n_iter = 1000,\n",
    "                                      scoring = 'neg_root_mean_squared_error', n_jobs = -1, cv = 10,\n",
    "                                      random_state = 1001)\n",
    "\n",
    "start_time = timer(None) # aktualna godzina\n",
    "svr_random_search.fit(X_train, Y_train)\n",
    "timer(start_time) # ile czasu minęło od start_time\n",
    "print(svr_random_search.best_score_)\n",
    "print(svr_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Porównanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bez strojenia</td>\n",
       "      <td>909.021810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Strojenie</td>\n",
       "      <td>793.275074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strojenie + StandradScaler</td>\n",
       "      <td>158.839379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model        RMSE\n",
       "0               Bez strojenia  909.021810\n",
       "1                   Strojenie  793.275074\n",
       "2  Strojenie + StandradScaler  158.839379"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_model = makePipeline(cat, num, svm.SVR(), ce.OneHotEncoder(), False)\n",
    "tuned_model = svr_random_search.best_estimator_\n",
    "scaled_model = svr_scaled_random_search.best_estimator_\n",
    "\n",
    "default_model.fit(X_train, Y_train)\n",
    "tuned_model.fit(X_train, Y_train)\n",
    "scaled_model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_default = default_model.predict(X_test)\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "y_pred_scaled = scaled_model.predict(X_test)\n",
    "\n",
    "mean_squared_error\n",
    "pd.DataFrame({\"Model\" : [\"Bez strojenia\", \"Strojenie\", \"Strojenie + StandradScaler\"],\n",
    "             \"RMSE\": [mean_squared_error(Y_test, y_pred_default, squared = False),\n",
    "                      mean_squared_error(Y_test, y_pred_tuned, squared = False),\n",
    "                      mean_squared_error(Y_test, y_pred_scaled, squared = False)]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zbiór danych \"forestfires\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   X       517 non-null    int64  \n",
      " 1   Y       517 non-null    int64  \n",
      " 2   month   517 non-null    object \n",
      " 3   day     517 non-null    object \n",
      " 4   FFMC    517 non-null    float64\n",
      " 5   DMC     517 non-null    float64\n",
      " 6   DC      517 non-null    float64\n",
      " 7   ISI     517 non-null    float64\n",
      " 8   temp    517 non-null    float64\n",
      " 9   RH      517 non-null    int64  \n",
      " 10  wind    517 non-null    float64\n",
      " 11  rain    517 non-null    float64\n",
      " 12  area    517 non-null    float64\n",
      "dtypes: float64(8), int64(3), object(2)\n",
      "memory usage: 52.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"forestfires.csv\")\n",
    "target = \"area\"\n",
    "predictors = [i for i in df.columns if i not in [target]]\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Sprawdzenie konieczności imputacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Brak braków danych mozemy przejść do encodingu zmiennych kategorycznych\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Podział danych na zbiór testowy oraz treningowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdzmy jak duzo jest uniklanych wartości w kolumnei district\n",
    "dtrain[\"district\"].unique()\n",
    "# Jest ich na tyle nieduzo, ze mozęmy skorzystać z OneHotEncodingu\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.drop(target, axis=1), df[target])\n",
    "\n",
    "# Kolumny z danymi numerycznymi\n",
    "num = X_train.select_dtypes(include=['float64', 'int']).columns\n",
    "\n",
    "# Kolumny z danymi kategorycznymi\n",
    "cat = X_train.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. SVR ze StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 6 minutes and 16.67 seconds.\n",
      "-45.98750420537989\n",
      "{'classifier__gamma': 'scale', 'classifier__degree': 9, 'classifier__C': 21.01}\n"
     ]
    }
   ],
   "source": [
    "# Stworzenei Pipeline'a z domyślnym klasyfiaktorem\n",
    "svr_classifier = svm.SVR()\n",
    "svr_scaled_pipeline = makePipeline(cat, num, svr_classifier, ce.OneHotEncoder(), True)\n",
    "\n",
    "# Jako metryke wzgledem ktorej bedziemy powownywac przyjmiemy RMSE\n",
    "svr_scaled_random_search = RandomizedSearchCV(svr_scaled_pipeline, param_distributions = params, n_iter = 1000,\n",
    "                                      scoring = 'neg_root_mean_squared_error', n_jobs = -1, cv = 10,\n",
    "                                      random_state = 1001)\n",
    "\n",
    "start_time = timer(None) # aktualna godzina\n",
    "svr_scaled_random_search.fit(X_train, Y_train)\n",
    "timer(start_time) # ile czasu minęło od start_time\n",
    "print(svr_scaled_random_search.best_score_)\n",
    "print(svr_scaled_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. SVR bez StandradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Time taken: 0 hours 3 minutes and 44.55 seconds.\n",
      "-45.56375710761044\n",
      "{'classifier__gamma': 'auto', 'classifier__degree': 8, 'classifier__C': 10.01}\n"
     ]
    }
   ],
   "source": [
    "# Stworzenei Pipeline'a z domyślnym klasyfiaktorem\n",
    "svr_classifier = svm.SVR()\n",
    "svr_pipeline = makePipeline(cat, num, svr_classifier, ce.OneHotEncoder(), False)\n",
    "\n",
    "# Jako metryke wzgledem ktorej bedziemy powownywac przyjmiemy RMSE\n",
    "svr_random_search = RandomizedSearchCV(svr_pipeline, param_distributions = params, n_iter = 1000,\n",
    "                                      scoring = 'neg_root_mean_squared_error', n_jobs = -1, cv = 10,\n",
    "                                      random_state = 1001)\n",
    "\n",
    "start_time = timer(None) # aktualna godzina\n",
    "svr_random_search.fit(X_train, Y_train)\n",
    "timer(start_time) # ile czasu minęło od start_time\n",
    "print(svr_random_search.best_score_)\n",
    "print(svr_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Porównanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bez strojenia</td>\n",
       "      <td>32.625247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Strojenie</td>\n",
       "      <td>32.090364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Strojenie + StandradScaler</td>\n",
       "      <td>32.265921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model       RMSE\n",
       "0               Bez strojenia  32.625247\n",
       "1                   Strojenie  32.090364\n",
       "2  Strojenie + StandradScaler  32.265921"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_model = makePipeline(cat, num, svm.SVR(), ce.OneHotEncoder(), False)\n",
    "tuned_model = svr_random_search.best_estimator_\n",
    "scaled_model = svr_scaled_random_search.best_estimator_\n",
    "\n",
    "default_model.fit(X_train, Y_train)\n",
    "tuned_model.fit(X_train, Y_train)\n",
    "scaled_model.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_default = default_model.predict(X_test)\n",
    "y_pred_tuned = tuned_model.predict(X_test)\n",
    "y_pred_scaled = scaled_model.predict(X_test)\n",
    "\n",
    "mean_squared_error\n",
    "pd.DataFrame({\"Model\" : [\"Bez strojenia\", \"Strojenie\", \"Strojenie + StandradScaler\"],\n",
    "             \"RMSE\": [mean_squared_error(Y_test, y_pred_default, squared = False),\n",
    "                      mean_squared_error(Y_test, y_pred_tuned, squared = False),\n",
    "                      mean_squared_error(Y_test, y_pred_scaled, squared = False)]\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Podsumowanie wyników"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Zbiór \"apartments\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To co momentalnie rzuca się w oczy to fakt, że skalowanie danych za pomocą funkcji StandradScaler() zdecydowanie poprawiłu rezultaty - znacząco zmniejszyło RMSE. Jeżeli chodzi o samą wartość RMSE można powiedzieć, że jest ona satysfakcjonująca. Biorąc pod uwagę rząd wielkości zmiennej celu, której średnia wynosi 3587, a odchylenei standradowe 906, to wynik po skalowaniu danych jest akceptowalny. Kolejna rzecz, którą można zauważyc to fakt, że podczas RandomizedSearchCV() dla danych nieskalowanych została wzięta największa możliwa wartość parametru c, co oznacza, że prawdopodobnie, gdyby zwiększyć jeszcze zakres to większa wartość C mogłaby dać jeszcze lepsze rezultaty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Zbiór \"forestfire\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeżeli chodzi o zbiór forestfire to okazał się on bardzo nietrafiony dla tego modelu. Sam zbió© danych jest dość ubogi - zawiera tylko 517 wierszy. Ponadto zmienan celu jest \"niezbalanasowana\", bo mimo, że jest zmienną ciagłą to i tak ponad ćwierć jej obserwacji wynosi 0 - co za tym idzie jest bardzo spore odchylenei standardowe. Myślę, ze te m.in. te 2 rzeczy są odpowiedzialne za marne wyniki tego algorytmu. Co ciekawe nie widać róznic w algorytmie dla danych przeskalowanych, a tych nieprzesaklowanych. Wynik jest mało satysfakcjonujący, biorąc pod uwagę, że średnia zmiennej celu to zaledwie 12.84. ZArówno jest to przykład bardzo trudnego zbioru do analizy, a zarazem takiego, na którym metoda SVR nie jest zbyt wydajna."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
